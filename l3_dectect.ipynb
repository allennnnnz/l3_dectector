{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dl2024\\AppData\\Local\\Temp\\ipykernel_14256\\120611879.py:9: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import STL10\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "CHECKPOINT_PATH = \"../saved_models\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定義data set\n",
    "## Labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(Dataset):\n",
    "\n",
    "    def __init__(self, contrast_root_dir, label_dir,transform):\n",
    "        self.contrast_root_dir = contrast_root_dir#根資料夾名\n",
    "        self.label_dir = label_dir#Label資料夾名\n",
    "        self.image_dir_path = os.path.join(self.contrast_root_dir, self.label_dir)#組合出圖片資料夾地址\n",
    "        #讀出個資料夾為一個列表\n",
    "        self.image_list = os.listdir(self.image_dir_path)#將圖片資料夾中的內容形成一個列表\n",
    "        self.image_list.sort()\n",
    "        self.transform = transform\n",
    "\n",
    "    #取得某張圖片\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_list[idx]#圖片檔名\n",
    "        img_item_path = os.path.join(self.contrast_root_dir, self.label_dir, img_name)#定位圖片完整地址\n",
    "        img = Image.open(img_item_path)\n",
    "        label = self.label_dir#label資料夾名就是圖片對應的label\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img , label\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class unlabel_MyData(Dataset):\n",
    "\n",
    "    def __init__(self, contrast_root_dir, label_dir,transform):\n",
    "        self.contrast_root_dir = contrast_root_dir#根資料夾名\n",
    "        self.label_dir = label_dir#Label資料夾名\n",
    "        self.image_dir_path = os.path.join(self.contrast_root_dir, self.label_dir)#組合出圖片資料夾地址\n",
    "        #讀出個資料夾為一個列表\n",
    "        self.image_list = os.listdir(self.image_dir_path)#將圖片資料夾中的內容形成一個列表\n",
    "        self.image_list.sort()\n",
    "        self.transform = transform\n",
    "\n",
    "    #取得某張圖片\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_list[idx]#圖片檔名\n",
    "        img_item_path = os.path.join(self.contrast_root_dir, self.label_dir, img_name)#定位圖片完整地址\n",
    "        img = Image.open(img_item_path)\n",
    "        label = -1\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return img , label\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定義argumentation過程\n",
    "## n_view代表要經過幾次transform過程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveTransformations(object):\n",
    "    \n",
    "    def __init__(self, base_transforms, n_views=2):\n",
    "        self.base_transforms = base_transforms\n",
    "        self.n_views = n_views\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        return [self.base_transforms(x) for i in range(self.n_views)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
    "                                          transforms.RandomApply([\n",
    "                                              transforms.ColorJitter(brightness=0.5, \n",
    "                                                                     contrast=0.5, \n",
    "                                                                     saturation=0.5, \n",
    "                                                                     hue=0.1)\n",
    "                                          ], p=0.8),\n",
    "                                          transforms.RandomGrayscale(p=0.2),\n",
    "                                          transforms.GaussianBlur(kernel_size=9),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize((0.5,), (0.5,))\n",
    "                                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_root_dir = \"E:/run_dataset_totalseg/train\"\n",
    "label_contrast_root_dir = \"E:/run_dataset_totalseg/valid\"\n",
    "L0_label_dir = 'L0'\n",
    "L1_label_dir = 'L1'\n",
    "L1_L2_label_dir = 'L1_L2'\n",
    "L2_label_dir = 'L2'\n",
    "L2_L3_label_dir = 'L2_L3'\n",
    "L3_label_dir = 'L3'\n",
    "L3_L4_label_dir = 'L3_L4'\n",
    "L4_label_dir = 'L4'\n",
    "L4_L5_label_dir = 'L4_L5'\n",
    "L5_label_dir = 'L5'\n",
    "T12_label_dir = 'T12'\n",
    "T12_L1_label_dir = 'T12_L1'\n",
    "unlabel = -1\n",
    "\n",
    "#train contrast\n",
    "contrast_L0_dataset = MyData(contrast_root_dir,L0_label_dir,transform=contrast_transforms)\n",
    "contrast_L1_dataset = MyData(contrast_root_dir,L1_label_dir,transform=contrast_transforms)\n",
    "contrast_L1_L2_dataset = MyData(contrast_root_dir,L1_L2_label_dir,transform=contrast_transforms)\n",
    "contrast_L2_dataset = MyData(contrast_root_dir,L2_label_dir,transform=contrast_transforms)\n",
    "contrast_L2_L3_dataset = MyData(contrast_root_dir,L2_L3_label_dir,transform=contrast_transforms)\n",
    "contrast_L3_dataset = MyData(contrast_root_dir,L3_label_dir,transform=contrast_transforms)\n",
    "contrast_L3_L4_dataset = MyData(contrast_root_dir,L3_L4_label_dir,transform=contrast_transforms)\n",
    "contrast_L4_dataset = MyData(contrast_root_dir,L4_label_dir,transform=contrast_transforms)\n",
    "contrast_L4_L5_dataset = MyData(contrast_root_dir,L4_L5_label_dir,transform=contrast_transforms)\n",
    "contrast_L5_dataset = MyData(contrast_root_dir,L5_label_dir,transform=contrast_transforms)\n",
    "contrast_T12_dataset = MyData(contrast_root_dir,T12_label_dir,transform=contrast_transforms)\n",
    "contrast_T12_L1_dataset = MyData(contrast_root_dir,T12_L1_label_dir,transform=contrast_transforms)\n",
    "\n",
    "contrast_other_dataset = contrast_L0_dataset + contrast_L1_dataset + contrast_L1_L2_dataset + contrast_L2_dataset + contrast_L2_L3_dataset + contrast_L3_L4_dataset + contrast_L4_dataset + contrast_L4_L5_dataset + contrast_L5_dataset + contrast_T12_dataset + contrast_T12_L1_dataset\n",
    "contrast_all_dataset = contrast_L3_dataset + contrast_other_dataset\n",
    "\n",
    "#unlabeled_data\n",
    "unlabeled_L0_dataset = unlabel_MyData(label_contrast_root_dir,L0_label_dir,transform=contrast_transforms)\n",
    "unlabeled_L1_dataset = unlabel_MyData(label_contrast_root_dir,L1_label_dir,transform=contrast_transforms)\n",
    "unlabeled_L1_L2_dataset = unlabel_MyData(label_contrast_root_dir,L1_L2_label_dir,transform=contrast_transforms)\n",
    "unlabeled_L2_dataset = unlabel_MyData(label_contrast_root_dir,L2_label_dir,transform=contrast_transforms)\n",
    "unlabeled_L2_L3_dataset = unlabel_MyData(label_contrast_root_dir,L2_L3_label_dir,transform=contrast_transforms)\n",
    "unlabeled_L3_dataset = unlabel_MyData(label_contrast_root_dir,L3_label_dir,transform=contrast_transforms)\n",
    "unlabeled_L3_L4_dataset = unlabel_MyData(label_contrast_root_dir,L3_L4_label_dir,transform=contrast_transforms)\n",
    "unlabeled_L4_dataset = unlabel_MyData(label_contrast_root_dir,L4_label_dir,transform=contrast_transforms)\n",
    "unlabeled_L4_L5_dataset = unlabel_MyData(label_contrast_root_dir,L4_L5_label_dir,transform=contrast_transforms)\n",
    "unlabeled_L5_dataset = unlabel_MyData(label_contrast_root_dir,L5_label_dir,transform=contrast_transforms)\n",
    "unlabeled_T12_dataset = unlabel_MyData(label_contrast_root_dir,T12_label_dir,transform=contrast_transforms)\n",
    "unlabeled_T12_L1_dataset = unlabel_MyData(label_contrast_root_dir,T12_L1_label_dir,transform=contrast_transforms)\n",
    "\n",
    "unlabeled_other_dataset = unlabeled_L0_dataset + unlabeled_L1_dataset + unlabeled_L1_L2_dataset + unlabeled_L2_dataset + unlabeled_L2_L3_dataset + unlabeled_L3_L4_dataset + unlabeled_L4_dataset + unlabeled_L4_L5_dataset + unlabeled_L5_dataset + unlabeled_T12_dataset + unlabeled_T12_L1_dataset\n",
    "unlabeled_all_dataset = unlabeled_L3_dataset + unlabeled_other_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定義simclr結構\n",
    "## resnet18 + projectionhead(full + relu + full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLR(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, hidden_dim, lr, temperature, weight_decay, max_epochs=500):\n",
    "        super().__init__()#繼承原module的所有init\n",
    "        self.save_hyperparameters()#記錄下每次訓練之後的參數，方便在多次訓練中去選出表現最好得模型\n",
    "        assert self.hparams.temperature > 0.0, 'The temperature must be a positive float!'\n",
    "        # Base model f(.)\n",
    "        self.convnet = torchvision.models.resnet18(num_classes=4*hidden_dim)  # Output of last linear layer\n",
    "        # The MLP for g(.) consists of Linear->ReLU->Linear \n",
    "        self.convnet.fc = nn.Sequential(\n",
    "            self.convnet.fc,  # Linear(ResNet output, 4*hidden_dim)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4*hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), \n",
    "                                lr=self.hparams.lr, \n",
    "                                weight_decay=self.hparams.weight_decay)\n",
    "        lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                            T_max=self.hparams.max_epochs,\n",
    "                                                            eta_min=self.hparams.lr/50)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "        \n",
    "    def info_nce_loss(self, batch, mode='train'):\n",
    "        imgs, _ = batch#只有給圖片沒有給label\n",
    "        imgs = torch.cat(imgs, dim=0)\n",
    "        \n",
    "        # Encode all images\n",
    "        feats = self.convnet(imgs)\n",
    "        # Calculate cosine similarity\n",
    "        cos_sim = F.cosine_similarity(feats[:,None,:], feats[None,:,:], dim=-1)\n",
    "        # Mask out cosine similarity to itself\n",
    "        self_mask = torch.eye(cos_sim.shape[0], dtype=torch.bool, device=cos_sim.device)\n",
    "        cos_sim.masked_fill_(self_mask, -9e15)\n",
    "        # Find positive example -> batch_size//2 away from the original example\n",
    "        pos_mask = self_mask.roll(shifts=cos_sim.shape[0]//2, dims=0)\n",
    "        # InfoNCE loss\n",
    "        cos_sim = cos_sim / self.hparams.temperature\n",
    "        nll = -cos_sim[pos_mask] + torch.logsumexp(cos_sim, dim=-1)\n",
    "        nll = nll.mean()\n",
    "        \n",
    "        # Logging loss\n",
    "        self.log(mode+'_loss', nll)\n",
    "        # Get ranking position of positive example\n",
    "        comb_sim = torch.cat([cos_sim[pos_mask][:,None],  # First position positive example\n",
    "                              cos_sim.masked_fill(pos_mask, -9e15)], \n",
    "                             dim=-1)\n",
    "        sim_argsort = comb_sim.argsort(dim=-1, descending=True).argmin(dim=-1)\n",
    "        # Logging ranking metrics\n",
    "        self.log(mode+'_acc_top1', (sim_argsort == 0).float().mean())\n",
    "        self.log(mode+'_acc_top5', (sim_argsort < 5).float().mean())\n",
    "        self.log(mode+'_acc_mean_pos', 1+sim_argsort.float().mean())\n",
    "        \n",
    "        return nll\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.info_nce_loss(batch, mode='train')\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.info_nce_loss(batch, mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simclr(batch_size, max_epochs=500, **kwargs):\n",
    "    trainer = pl.Trainer(default_root_dir=os.path.join(CHECKPOINT_PATH, 'SimCLR'),\n",
    "                         accelerator=\"gpu\" if str(device).startswith(\"cuda\") else \"cpu\",\n",
    "                         devices=1,\n",
    "                         benchmark=True,\n",
    "                         profiler='simple',\n",
    "                         max_epochs=max_epochs,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode='max', monitor='val_acc_top5'),\n",
    "                                    LearningRateMonitor('epoch')])\n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, 'SimCLR.ckpt')\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(f'Found pretrained model at {pretrained_filename}, loading...')\n",
    "        model = SimCLR.load_from_checkpoint(pretrained_filename) # Automatically loads the model with the saved hyperparameters\n",
    "    else:\n",
    "        train_loader = data.DataLoader(unlabeled_all_dataset, batch_size=batch_size, shuffle=True, \n",
    "                                       drop_last=True, pin_memory=True, num_workers=4)\n",
    "        val_loader = data.DataLoader(contrast_all_dataset, batch_size=batch_size, shuffle=False, \n",
    "                                     drop_last=False, pin_memory=True, num_workers=4)\n",
    "        pl.seed_everything(42) # To be reproducable\n",
    "        model = SimCLR(max_epochs=max_epochs, **kwargs)\n",
    "        trainer.fit(model, train_loader, val_loader)\n",
    "        model = SimCLR.load_from_checkpoint(trainer.checkpoint_callback.best_model_path) # Load best checkpoint after training\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type   | Params\n",
      "-----------------------------------\n",
      "0 | convnet | ResNet | 11.5 M\n",
      "-----------------------------------\n",
      "11.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.5 M    Total params\n",
      "46.019    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88739dbac9d348cf883b44f9105f28ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    }
   ],
   "source": [
    "simclr_model = train_simclr(batch_size=4096, \n",
    "                            hidden_dim=128, \n",
    "                            lr=5e-4, \n",
    "                            temperature=0.07, \n",
    "                            weight_decay=1e-4, \n",
    "                            max_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full path E:/run_dataset_totalseg/train\\L3\\s0001_140_L3.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "contrast_root_dir = \"E:/run_dataset_totalseg/train\"\n",
    "label_dir = 'L3'\n",
    "image_path = os.path.join(contrast_root_dir,label_dir)\n",
    "img_list = os.listdir(image_path)\n",
    "fullpath = os.path.join(image_path,img_list[0])\n",
    "print(\"full path\",fullpath)\n",
    "img = Image.open(fullpath)\n",
    "img.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
